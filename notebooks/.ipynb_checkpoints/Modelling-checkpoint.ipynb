{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbc0af4c-ddf6-4729-8cea-952f3c7d1104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\luiz.santos\\documents\\fiap grupo\\tech challenge\\.venv\\lib\\site-packages (2.2.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\luiz.santos\\documents\\fiap grupo\\tech challenge\\.venv\\lib\\site-packages (1.4.1.post1)\n",
      "Requirement already satisfied: optuna in c:\\users\\luiz.santos\\documents\\fiap grupo\\tech challenge\\.venv\\lib\\site-packages (3.6.1)\n",
      "Requirement already satisfied: statsmodels in c:\\users\\luiz.santos\\documents\\fiap grupo\\tech challenge\\.venv\\lib\\site-packages (0.14.1)\n",
      "Requirement already satisfied: scikit-optimize in c:\\users\\luiz.santos\\documents\\fiap grupo\\tech challenge\\.venv\\lib\\site-packages (0.10.1)\n",
      "Requirement already satisfied: numpy<2,>=1.26.0 in c:\\users\\luiz.santos\\documents\\fiap grupo\\tech challenge\\.venv\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\luiz.santos\\documents\\fiap grupo\\tech challenge\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\luiz.santos\\documents\\fiap grupo\\tech challenge\\.venv\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\luiz.santos\\documents\\fiap grupo\\tech challenge\\.venv\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\luiz.santos\\documents\\fiap grupo\\tech challenge\\.venv\\lib\\site-packages (from scikit-learn) (1.13.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\luiz.santos\\documents\\fiap grupo\\tech challenge\\.venv\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\luiz.santos\\documents\\fiap grupo\\tech challenge\\.venv\\lib\\site-packages (from scikit-learn) (3.4.0)\n",
      "Requirement already satisfied: alembic>=1.5.0 in c:\\users\\luiz.santos\\documents\\fiap grupo\\tech challenge\\.venv\\lib\\site-packages (from optuna) (1.13.1)\n",
      "Requirement already satisfied: colorlog in c:\\users\\luiz.santos\\documents\\fiap grupo\\tech challenge\\.venv\\lib\\site-packages (from optuna) (6.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\luiz.santos\\documents\\fiap grupo\\tech challenge\\.venv\\lib\\site-packages (from optuna) (24.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in c:\\users\\luiz.santos\\documents\\fiap grupo\\tech challenge\\.venv\\lib\\site-packages (from optuna) (2.0.29)\n",
      "Requirement already satisfied: tqdm in c:\\users\\luiz.santos\\documents\\fiap grupo\\tech challenge\\.venv\\lib\\site-packages (from optuna) (4.66.2)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\luiz.santos\\documents\\fiap grupo\\tech challenge\\.venv\\lib\\site-packages (from optuna) (6.0.1)\n",
      "Requirement already satisfied: patsy>=0.5.4 in c:\\users\\luiz.santos\\documents\\fiap grupo\\tech challenge\\.venv\\lib\\site-packages (from statsmodels) (0.5.6)\n",
      "Requirement already satisfied: pyaml>=16.9 in c:\\users\\luiz.santos\\documents\\fiap grupo\\tech challenge\\.venv\\lib\\site-packages (from scikit-optimize) (23.12.0)\n",
      "Requirement already satisfied: Mako in c:\\users\\luiz.santos\\documents\\fiap grupo\\tech challenge\\.venv\\lib\\site-packages (from alembic>=1.5.0->optuna) (1.3.2)\n",
      "Requirement already satisfied: typing-extensions>=4 in c:\\users\\luiz.santos\\documents\\fiap grupo\\tech challenge\\.venv\\lib\\site-packages (from alembic>=1.5.0->optuna) (4.11.0)\n",
      "Requirement already satisfied: six in c:\\users\\luiz.santos\\documents\\fiap grupo\\tech challenge\\.venv\\lib\\site-packages (from patsy>=0.5.4->statsmodels) (1.16.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\luiz.santos\\documents\\fiap grupo\\tech challenge\\.venv\\lib\\site-packages (from sqlalchemy>=1.3.0->optuna) (3.0.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\luiz.santos\\documents\\fiap grupo\\tech challenge\\.venv\\lib\\site-packages (from colorlog->optuna) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\users\\luiz.santos\\documents\\fiap grupo\\tech challenge\\.venv\\lib\\site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas scikit-learn optuna statsmodels scikit-optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88537431-7a90-4c7c-a1d0-5eeb475c641e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "import os\n",
    "import logging\n",
    "from include.utils import incrementar_dados_aleatorios_csv, limpar_dados, categorizar_imc, check_modelo_nan_inf\n",
    "from include.utils import prever_encargos_futuros, segmentacao_de_risco, analise_de_sensibilidade, otimizacao_de_recursos, planejamento_estrategico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2f178e3-93ea-45ca-8c37-9aeb74ea7184",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'BayesSearchCV' from 'sklearn.model_selection' (C:\\Users\\luiz.santos\\Documents\\FIAP Grupo\\Tech Challenge\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mskopt\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspace\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Real, Categorical, Integer\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split, cross_val_score, BayesSearchCV\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomForestRegressor, GradientBoostingRegressor\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LinearRegression, Lasso\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'BayesSearchCV' from 'sklearn.model_selection' (C:\\Users\\luiz.santos\\Documents\\FIAP Grupo\\Tech Challenge\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320499db-f973-450a-9aab-316e6426be3a",
   "metadata": {},
   "source": [
    "**1. Engenharia de Recursos**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a110e58-c228-4985-a256-24ffc5dcc631",
   "metadata": {},
   "source": [
    "Novos recursos derivados dos dados existentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b10de9-2b41-4f53-b88d-2d46be3eb795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtém a largura do terminal - uso mais embaixo nos prints\n",
    "terminal_width = os.get_terminal_size().columns\n",
    "\n",
    "# Carregar os dados processados\n",
    "X_train = pd.read_csv('../planilhas/3_dados_processados_treino.csv', encoding='latin-1')\n",
    "X_test = pd.read_csv('../planilhas/4_dados_processados_teste.csv', encoding='latin-1')\n",
    "y_train = pd.read_csv('../planilhas/5_dados_processados_treino_target.csv', encoding='latin-1')\n",
    "y_test = pd.read_csv('../planilhas/6_dados_processados_teste_target.csv', encoding='latin-1')\n",
    "\n",
    "# Converter y_train em um array unidimensional\n",
    "y_train = y_train.values.ravel()\n",
    "\n",
    "# Converter y_test em um array unidimensional\n",
    "y_test = y_test.values.ravel()\n",
    "\n",
    "# Descarto na predição do modelo, mas trago novamente nos dados futuros\n",
    "colunas_descartadas_treino = X_train[['Gênero','Região','Categoria_IMC']]\n",
    "colunas_descartadas_teste = X_test[['Gênero','Região','Categoria_IMC']]\n",
    "X_train = X_train.drop(colunas_descartadas_treino, axis=1)\n",
    "X_test = X_test.drop(colunas_descartadas_teste, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebb4b2f-58ca-4fa7-97d1-af96eb7d266d",
   "metadata": {},
   "source": [
    "**2. Limpeza de Dados**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be726d7-f49f-4115-9c22-9bf83b87f4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = limpar_dados(X_train)\n",
    "X_test = limpar_dados(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7927a3-d421-4266-a465-4a60ea793d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nQuantidade de valores ausentes por coluna:\\n\\n\", X_train.isnull().sum())\n",
    "print(f\"\\nQuantidade de linhas: {X_train.shape[0]}. Quantidade de colunas: {X_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de911716-dd4a-485b-80c8-faeb34648740",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nQuantidade de valores ausentes por coluna:\\n\\n\", X_test.isnull().sum())\n",
    "print(f\"\\nQuantidade de linhas: {X_test.shape[0]}. Quantidade de colunas: {X_test.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad26fe86-4dd4-488f-90ee-3a86ac237459",
   "metadata": {},
   "source": [
    "**3. Pré-processamento**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17d1075-c010-4b2d-bb6a-1f63dc89cd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar o StandardScaler, que é usado para padronizar os recursos\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Aplicar a transformação de padronização nos dados de treinamento e ajustar o scaler aos dados\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Aplicar a mesma transformação de padronização aos dados de teste\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Calcular a matriz de correlação dos dados de treinamento\n",
    "correlation_matrix = X_train.corr()\n",
    "print('Visualizar a matriz de correlação:')\n",
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464d0f57-fd29-48b2-a59e-42e36e34e72d",
   "metadata": {},
   "source": [
    "**Validação estatística**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64155550-918c-4280-b673-b8eb7e920084",
   "metadata": {},
   "source": [
    "Utilizando métricas estatísticas para validar a eficácia do modelo (p-value, intervalos de confiança):            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e44b40-d5be-4983-8bf9-ed753892fcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dados_aux = X_train.copy()\n",
    "\n",
    "#dados_aux['Fumante'] = dados_aux['Fumante'].map({'sim': 1, 'não': 0})\n",
    "\n",
    "# Adicione uma coluna de intercepto aos dados\n",
    "#dados_aux['Intercepto'] = 1\n",
    "\n",
    "# Dividindo os dados em variáveis independentes (X) e variável dependente (y)\n",
    "#X = dados_aux[['Idade', 'Filhos', 'Fumante']]\n",
    "#y = dados_aux['Encargos']\n",
    "\n",
    "# Ajustando o modelo de regressão linear usando OLS (Ordinary Least Squares)\n",
    "#modelo = sm.OLS(X_test, X_train).fit()\n",
    "\n",
    "# Calculando os p-values dos coeficientes\n",
    "#p_values = modelo.summary2().tables[1]['P>|t|']\n",
    "\n",
    "# Calculando os intervalos de confiança dos coeficientes\n",
    "#conf_intervals = modelo.conf_int()\n",
    "\n",
    "# Imprima os resultados\n",
    "#print(\"P-values dos coeficientes:\")\n",
    "#print(p_values)\n",
    "#print(\"\\nIntervalos de Confiança dos coeficientes:\")\n",
    "#print(round(conf_intervals,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97c5862-6ad0-4e20-9e49-a447bfaa48c2",
   "metadata": {},
   "source": [
    "**4. Treinamento do Modelo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2204d8a0-b97e-4ce2-a905-78ff8791deab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adicionando mais modelos ao dicionário de modelos\n",
    "models = {\n",
    "    'Random Forest Regressor': RandomForestRegressor(),\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Support Vector Regression': SVR(),\n",
    "    'Decision Tree Regressor': DecisionTreeRegressor(),\n",
    "    'K-Nearest Neighbors Regressor': KNeighborsRegressor(),\n",
    "    'Gradient Boosting Regressor': GradientBoostingRegressor(),\n",
    "    'Lasso Regression': Lasso()\n",
    "}\n",
    "\n",
    "# Definindo os espaços de busca para os hiperparâmetros de cada modelo\n",
    "param_spaces = {\n",
    "    'Random Forest Regressor': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': list(range(1, 11))  # Corrigindo aqui\n",
    "    },\n",
    "    'Support Vector Regression': {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'gamma': ['scale', 'auto']\n",
    "    },\n",
    "    'Decision Tree Regressor': {\n",
    "        'max_depth': list(range(1, 11))  # Corrigindo aqui\n",
    "    },\n",
    "    'K-Nearest Neighbors Regressor': {\n",
    "        'n_neighbors': list(range(3, 11))  # Corrigindo aqui\n",
    "    },\n",
    "    'Gradient Boosting Regressor': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': list(range(1, 11))  # Corrigindo aqui\n",
    "    },\n",
    "    'Lasso Regression': {\n",
    "        'alpha': [0.1, 1, 10]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be67531c-1538-4d5e-858f-7ab5406ffd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# usado no método abaixo BayesSearchCV (como demora um pouco, caso seja necessário, basta reduzir a quantidade de interações\n",
    "numero_interacoes_bayes_search = 2\n",
    "\n",
    "# Treinamento e avaliação dos modelos com BayesSearchCV\n",
    "for name, model in models.items():\n",
    "    # Imprime uma linha separadora com base na largura do terminal\n",
    "    print('-' * terminal_width)\n",
    "    \n",
    "    try:\n",
    "        check_modelo_nan_inf(model, X_train, y_train)\n",
    "        print(f\"Modelo {name}: OK. Sem NaN e Inf nas previsões do modelo\")\n",
    "    except ValueError as e:\n",
    "        print(f\"{name}: {e}\")\n",
    "    \n",
    "    if name in param_spaces:\n",
    "        bayes_search = BayesSearchCV(model, param_spaces[name], cv=5, scoring='neg_mean_squared_error', n_iter=numero_interacoes_bayes_search, random_state=42)\n",
    "        bayes_search.fit(X_train, y_train)\n",
    "        model = bayes_search.best_estimator_\n",
    "        print(f\"Melhores parâmetros para {name}: {bayes_search.best_params_}\")\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "    print(f'{name} MSE: {round(mse,2)}')\n",
    "    # Calcular acurácia média usando validação cruzada\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=5)\n",
    "    print(f'Acurácia média com validação cruzada para {name}: {round(scores.mean(),2)}')\n",
    "        \n",
    "    # Calcula e exibe a importância das características para o modelo atual\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importance_df = pd.DataFrame({'Colunas': X_train.columns, 'Importância': model.feature_importances_})\n",
    "        importance_df = importance_df.sort_values(by='Importância', ascending=False)\n",
    "        print(f\"Importância das características para {name}:\")\n",
    "        print(round(importance_df,2))\n",
    "    elif isinstance(model, LinearRegression) or isinstance(model, Lasso):        \n",
    "        importance_df = pd.DataFrame({'Colunas': X_train.columns, 'Coeficiente': model.coef_})\n",
    "        importance_df = importance_df.sort_values(by='Coeficiente', ascending=False)\n",
    "        print(f\"Coeficientes das características para {name}:\")\n",
    "        print(round(importance_df,2))\n",
    "    elif isinstance(model, SVR):\n",
    "        support_indices = model.support_\n",
    "        support_features = X_train.iloc[support_indices]\n",
    "        feature_importance_svr = support_features.mean(axis=0)\n",
    "        importance_df = pd.DataFrame({'Colunas': X_train.columns, 'Importância': feature_importance_svr})\n",
    "        importance_df = importance_df.sort_values(by='Importância', ascending=False)\n",
    "        print(f\"Importância das características para {name}:\")\n",
    "        print(round(importance_df,2))\n",
    "    else:\n",
    "        print(f\"Não é possível calcular a importância das características para o modelo {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fb4ff9-f2ce-4114-bc95-28f313d6690f",
   "metadata": {},
   "source": [
    "**5. Comparação de Modelos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955fbdea-faea-4285-8298-76b32a8bd648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleção de modelo com base na maior pontuação de validação cruzada\n",
    "best_model_name = max(models, key=lambda model: cross_val_score(models[model], X_train, y_train, cv=5).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9684717-7ac2-4c36-a697-1f3a72d85d51",
   "metadata": {},
   "source": [
    "**6. Seleção de Modelo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16947f1b-a506-4540-9068-99f278e9d558",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = models[best_model_name]\n",
    "print(f'Best Model: {best_model_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2c235a-b688-4d6b-8820-de8a50e3e4a8",
   "metadata": {},
   "source": [
    "**7. Otimização de Modelo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36479d9-807c-4862-bcd6-a1b06426edce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para criar uma nova instância do modelo com hiperparâmetros definidos\n",
    "def create_model(trial):\n",
    "    if best_model_name == 'Random Forest Regressor':\n",
    "        n_estimators = trial.suggest_int('n_estimators', 10, 100)\n",
    "        max_depth = trial.suggest_int('max_depth', 2, 32)\n",
    "        min_samples_split = trial.suggest_int('min_samples_split', 2, 20)\n",
    "        min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)\n",
    "        model = RandomForestRegressor(n_estimators=n_estimators, \n",
    "                                       max_depth=max_depth, \n",
    "                                       min_samples_split=min_samples_split,\n",
    "                                       min_samples_leaf=min_samples_leaf)\n",
    "    elif best_model_name == 'Decision Tree Regressor':\n",
    "        max_depth = trial.suggest_int('max_depth', 2, 32)\n",
    "        min_samples_split = trial.suggest_int('min_samples_split', 2, 20)\n",
    "        min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)\n",
    "        model = DecisionTreeRegressor(max_depth=max_depth,\n",
    "                                       min_samples_split=min_samples_split,\n",
    "                                       min_samples_leaf=min_samples_leaf)\n",
    "    elif best_model_name == 'Support Vector Regression':\n",
    "        C = trial.suggest_loguniform('C', 1e-3, 1e3)\n",
    "        epsilon = trial.suggest_loguniform('epsilon', 1e-3, 1e1)\n",
    "        kernel = trial.suggest_categorical('kernel', ['linear', 'rbf', 'poly'])\n",
    "        model = SVR(C=C, epsilon=epsilon, kernel=kernel)\n",
    "    elif best_model_name == 'Linear Regression':\n",
    "        model = LinearRegression()\n",
    "    elif best_model_name == 'K-Nearest Neighbors Regressor':\n",
    "        n_neighbors = trial.suggest_int('n_neighbors', 1, 20)\n",
    "        model = KNeighborsRegressor(n_neighbors=n_neighbors)\n",
    "    elif best_model_name == 'Gradient Boosting Regressor':\n",
    "        learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 0.1)\n",
    "        n_estimators = trial.suggest_int('n_estimators', 50, 200)\n",
    "        max_depth = trial.suggest_int('max_depth', 3, 10)\n",
    "        model = GradientBoostingRegressor(learning_rate=learning_rate, \n",
    "                                           n_estimators=n_estimators, \n",
    "                                           max_depth=max_depth)\n",
    "    else:\n",
    "        model = models[best_model_name]  # Usar os hiperparâmetros padrão para outros modelos\n",
    "    return model\n",
    "\n",
    "# Função para otimização de hiperparâmetros\n",
    "def objective(trial):\n",
    "    # Criar uma nova instância do modelo com hiperparâmetros definidos pelo Optuna\n",
    "    model = create_model(trial)\n",
    "    \n",
    "    # Avaliação do modelo utilizando validação cruzada\n",
    "    score = cross_val_score(model, X_train, y_train, n_jobs=-1, cv=3, scoring='neg_mean_squared_error').mean()\n",
    "    return score\n",
    "\n",
    "# Criar o objeto de estudo Optuna\n",
    "study = optuna.create_study(direction='minimize')\n",
    "\n",
    "# Realizar a otimização dos hiperparâmetros\n",
    "study.optimize(objective, n_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c466152-7210-4ae6-bbe2-50bbd3fcd988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenha os melhores parâmetros do estudo Optuna\n",
    "best_params = study.best_params\n",
    "best_score = study.best_value\n",
    "\n",
    "# Configure o modelo com os melhores parâmetros\n",
    "best_model.set_params(**best_params)\n",
    "\n",
    "# Ajuste o modelo aos dados de treinamento\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Avaliação do modelo\n",
    "score_train = model.score(X_train, y_train)\n",
    "score_test = model.score(X_test, y_test)\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Imprimir resultados\n",
    "print(\"\\nMelhores parâmetros:\", best_params)\n",
    "print(\"\\nMelhor score:\", round(best_score,2))\n",
    "print(f\"\\nScore treino: {round(score_train,2)}\")\n",
    "print(f\"\\nScore teste: {round(score_test,2)}\")\n",
    "print(f'\\nOptimized {best_model_name} MSE: {round(mse,2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33547ecc-41db-4611-b112-781aa8ad267f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obter a importância das características apenas para modelos que suportam esse atributo\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    # Obter a importância das características do modelo\n",
    "    feature_importances = best_model.feature_importances_\n",
    "    \n",
    "    # Criar um DataFrame para visualizar a importância das características\n",
    "    importance_df = pd.DataFrame({'Colunas': X_train.columns, 'Importância': feature_importances})\n",
    "\n",
    "    # Ordenar as características por importância\n",
    "    importance_df = importance_df.sort_values(by='Importância', ascending=False)\n",
    "    \n",
    "    print(\"\\nQuais características individuais têm maior impacto nos custos médicos cobrados pelo seguro de saúde?\")\n",
    "    print(round(importance_df,2))\n",
    "\n",
    "    # Selecionar as características mais importantes (por exemplo, as 10 mais importantes)\n",
    "    top_features = importance_df.head(10)['Colunas'].tolist()  # Ajuste o número conforme necessário\n",
    "    \n",
    "    # Calcular a matriz de correlação apenas para as características mais importantes\n",
    "    correlation_matrix = X_train[top_features].corr()\n",
    "    \n",
    "    # Visualizar a matriz de correlação\n",
    "    print(\"\\nExiste alguma correlação entre certas características (por exemplo, idade, IMC) e os custos médicos?\")\n",
    "    print(correlation_matrix)\n",
    "else:\n",
    "    print(\"O modelo selecionado não suporta o cálculo de importância das características.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a03c62-b587-404f-a8bd-5f6ae92ba516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validação cruzada para detecção de overfitting\n",
    "def overfitting_detection(model, X, y, cv=5):\n",
    "    train_mse = -cross_val_score(model, X, y, cv=cv, scoring='neg_mean_squared_error').mean()\n",
    "    model.fit(X, y)\n",
    "    y_pred_train = model.predict(X)\n",
    "    test_mse = np.mean((y - y_pred_train) ** 2)\n",
    "    return train_mse, test_mse\n",
    "\n",
    "# Exemplo de uso\n",
    "train_mse, test_mse = overfitting_detection(best_model, X_train_scaled, y_train)\n",
    "\n",
    "print(\"MSE no conjunto de treinamento:\", round(train_mse,2))\n",
    "print(\"MSE no conjunto de teste:\", round(test_mse,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85eb16b-00c1-4b47-8a9c-1f6381779783",
   "metadata": {},
   "source": [
    "**8. Entrega dos dados futuros**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff98bdc-0c3b-47f8-8286-bfe6f7e6425d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definição de dados futuros - 200 linhas\n",
    "dados_futuros = pd.DataFrame(np.random.rand(200, len(X_test.columns)), columns=X_test.columns)\n",
    "print(dados_futuros.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6d667b-d351-4280-acc0-e3eadcd2691a",
   "metadata": {},
   "source": [
    "Modelo treinado para fazer previsões dos encargos futuros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef157340-e0fe-40af-99cf-c51dc182c8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "encargos_futuros = prever_encargos_futuros(best_model, dados_futuros)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a705801-4d0c-49a5-b8c2-0179edc79268",
   "metadata": {},
   "source": [
    "Grupos de indivíduos com diferentes níveis de risco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fca20a5-19a3-4b6e-813a-8e3bc3eff4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "grupos_risco = segmentacao_de_risco(best_model, dados_futuros)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f827ad-ea14-449f-ac30-740f7847e0ad",
   "metadata": {},
   "source": [
    "Análises de sensibilidade para entender o impacto das mudanças nas características dos segurados nos encargos previstos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e827e0d-72e5-4bbf-b458-7bab84abfdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "encargos_futuros_expectativa_40_anos = analise_de_sensibilidade(best_model, dados_futuros, 'Idade', novo_valor=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38da1be-74f3-4fc5-9ab3-da6447d943ef",
   "metadata": {},
   "source": [
    "Utilizando as previsões dos encargos para otimizar a alocação de recursos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3740e3ff-fcdf-416f-9aa4-3f54b5d2f539",
   "metadata": {},
   "outputs": [],
   "source": [
    "recursos_otimizados = otimizacao_de_recursos(encargos_futuros)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e3345c-a5d0-4a2c-aa64-7c675cf967d6",
   "metadata": {},
   "source": [
    "Utilizando as informações obtidas com o modelo para desenvolver planos estratégicos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160d53a4-3d2b-4a41-af3b-cbbbf91116c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "planos_estrategicos = planejamento_estrategico(best_model, dados_futuros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5651aad-ab93-4e5d-9119-62173d584776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserir as colunas descartadas em X_test\n",
    "for coluna in colunas_descartadas_treino.columns:\n",
    "    dados_futuros.insert(loc=len(dados_futuros.columns), column=coluna, value=colunas_descartadas_treino[coluna])\n",
    "\n",
    "# Adicionando as novas colunas na planilha dados futuros\n",
    "dados_futuros['Encargos Reais'] = np.nan # não consegui ainda obter (dúvida com o professor)\n",
    "dados_futuros['Encargo Futuro'] = encargos_futuros\n",
    "dados_futuros['Grupos Risco'] = grupos_risco\n",
    "dados_futuros['Encargo Futuro - Expectativa 40 anos'] = encargos_futuros_expectativa_40_anos\n",
    "dados_futuros['Otimização Recursos'] = recursos_otimizados\n",
    "dados_futuros['Planos estratégicos'] = planos_estrategicos\n",
    "\n",
    "# Salvar os dados randômicos futuros\n",
    "dados_futuros.to_csv(\"../planilhas/8_dados_futuros.csv\", index=False, encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34520248-7ad9-4789-88c9-d3e48b98ffcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(15, 5))\n",
    "\n",
    "plt.plot(dados_futuros['Encargo Futuro'], label='Previsões', color='blue', marker='o')\n",
    "plt.plot(dados_futuros['Encargos Reais'], label='Encargos Reais', color='green', marker='x')\n",
    "\n",
    "plt.xlabel('Previsões')\n",
    "plt.ylabel('Encargos')\n",
    "plt.legend()\n",
    "plt.title('Previsões vs. Encargos Reais')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
